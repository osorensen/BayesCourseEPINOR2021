---
title: "EPINOR Annual Meeting 2021"
subtitle: "Part III - Bayesian Analysis with R"
author: "Øystein Sørensen"
institute: "University of Oslo"
date: "2021/11/02"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(cache = TRUE)
```

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
theme_set(theme_bw())
```



# Outline for Part III

- An overview of tools for Bayesian analysis in R

- Stan, brms

- 15 minute break

- Hands-on exercises

---

# Fitting Bayesian Models

- Computation for Bayesian models requires more expertise than fitting frequentist models.

- [BUGS - Bayesian Inference Using Gibbs Sampling](https://www.mrc-bsu.cam.ac.uk/software/bugs/)
  
  - Early software, which you may encounter in older textbooks. Can be used, but no longer actively developed.
  
- [JAGS - Just Another Gibbs Sampler](https://mcmc-jags.sourceforge.io/)

  - Further development of BUGS, but no new releases last four years.
  
- [Stan](https://mc-stan.org/)

  - State-of-the-art tool for Bayesian computation.
  
  - Very active community.


---

# Fitting Bayesian Models

- Stan is a programming language of its own, definitely increasing the barrier for easy learning.

- Can use R packages that yield familiar syntax, translating R code into Stan code:

  - [rethinking](https://github.com/rmcelreath/rethinking)
  
  - [rstanarm](https://mc-stan.org/rstanarm/)
  
  - [brms](https://paul-buerkner.github.io/brms/)
  
- [You can also use Stan from Stata!](https://github.com/stan-dev/statastan)


---

# Stan?

.pull-left[
[Stanisław Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam)

> a Polish scientist in the fields of mathematics and nuclear physics. He participated in the Manhattan Project, originated the Teller–Ulam design of thermonuclear weapons, discovered the concept of the cellular automaton, invented the Monte Carlo method of computation, and suggested nuclear pulse propulsion
]
.pull-right[
<img src="figures/Stanislaw_Ulam.jpg" height=400>
]


---

# brms

```{r}
library(brms)
```


.footnote[
Bürkner, Paul-Christian. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80, no. 1 (August 29, 2017): 1–28. https://doi.org/10.18637/jss.v080.i01.
Bürkner, Paul-Christian. “Advanced Bayesian Multilevel Modeling with the R Package Brms.” The R Journal 10, no. 1 (2018): 395–411. https://doi.org/10.32614/RJ-2018-017.
]

---

# Fitting a logistic regression model with brms

```{r}
dat <- readRDS("data/bwt.rds")
head(dat)
```


---

# Fitting a logistic regression model with brms

```r
mod <- brm(Low ~ LWT, data = dat, family = bernoulli())
## Compiling Stan program...
## Start sampling
## 
## SAMPLING FOR MODEL 'd3c8392ac3fcb0c96e96bdaad5582435' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## ...
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.058381 seconds (Warm-up)
## Chain 4:                0.054892 seconds (Sampling)
## Chain 4:                0.113273 seconds (Total)
## Chain 4: 
```


---

# Fitting a logistic regression model with brms


```{r, eval=FALSE}
mod <- brm(Low ~ LWT, data = dat, family = bernoulli())
```

is very similar to

```{r, eval=FALSE}
mod_glm <- glm(Low ~ LWT, data = dat, family = binomial())
```

- `family = binomial()` would work with brms too, but is not the standard syntax.

---

# Model output

```{r, echo=FALSE, eval=FALSE}
mod <- brm(Low ~ LWT, data = dat, family = bernoulli())
saveRDS(mod, "data/mod.rds")
```

```{r, echo=FALSE}
mod <- readRDS("data/mod.rds")
```


```{r}
summary(mod)
```

---

class: inverse, middle, center

# Let's try to use the Bayesian workflow

---

# (a) Exploratory Data Analysis

- Important, but not particularly related to brms. 

---

# (b) Prior Distribution

In the previous lecture we decided on using a normal prior for $\beta_{LWT}$ with mean 0 and standard deviation 0.4.

We start by getting the default prior:

```{r}
prior <- get_prior(Low ~ LWT, family = bernoulli(), 
                   data = dat)
print(prior, show_df = FALSE)
```

- "b" denotes all population-level parameters
- "b_LWT" denotes the particular LWT parameter
- "Intercept" is what it is.

---

# (b) Prior Distribution

Update to get what we want:

```{r}
prior$prior[2] <- "normal(0, 0.4)"
```

--

Check that prior is updated:

```{r}
print(prior, show_df = FALSE)
```

---

# (c) Check that the computations work correctly

Then we have to fit the model first, with our preferred prior:

```{r, eval=FALSE}
mod <- brm(Low ~ LWT, data = dat, family = bernoulli(),
           prior = prior)
```


```{r, eval=FALSE, echo=FALSE}
mod <- brm(Low ~ LWT, data = dat, family = bernoulli(),
           prior = prior)
saveRDS(mod, "data/mod_prior.rds")
```

```{r, echo=FALSE}
mod <- readRDS("data/mod_prior.rds")
```


```
## Compiling Stan program...
```

```
## recompiling to avoid crashing R session
```

```
## Start sampling
```

```
## 
## SAMPLING FOR MODEL 'f53afefa5fd92239adec95c91ede857a' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.032954 seconds (Warm-up)
## Chain 1:                0.025207 seconds (Sampling)
## Chain 1:                0.058161 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'f53afefa5fd92239adec95c91ede857a' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.2e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.032529 seconds (Warm-up)
## Chain 2:                0.029446 seconds (Sampling)
## Chain 2:                0.061975 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'f53afefa5fd92239adec95c91ede857a' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.1e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.037319 seconds (Warm-up)
## Chain 3:                0.02989 seconds (Sampling)
## Chain 3:                0.067209 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'f53afefa5fd92239adec95c91ede857a' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.1e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.031868 seconds (Warm-up)
## Chain 4:                0.030694 seconds (Sampling)
## Chain 4:                0.062562 seconds (Total)
## Chain 4:
```


---

# (c) Check that computations work correctly

```{r, eval=TRUE, dev='svg', fig.height=4}
plot(mod)
```

---

# (c) Check that computations work correctly

```{r, eval=TRUE, dev='svg', fig.height=4}
plot(mod, combo = c("parcoord", "rank_overlay"))
```


---

# (c) Check that computations work correctly

```{r}
summary(mod)
```


---

# (d) Check that the model fits the data well

```{r, eval=TRUE, dev='svg', fig.height=4}
pp_check(mod)
```

---

# (d) Check that the model fits the data well

```{r, eval=TRUE, dev='svg', fig.height=4}
pp_check(mod, type = "error_hist", ndraws = 11)
```

---

# (d) Check that the model fits the data well

```{r, eval=TRUE, dev='svg', fig.height=4}
pp_check(mod, type = "scatter_avg")
```

---

# (d) Check that the model fits the data well

```{r, eval=TRUE, dev='svg', fig.height=4}
pp_check(mod, ndraws = 100, type = "bars")
```


---

# (e) Compare to other more or less complicated models

Extend the model by adding smoking status

```{r, eval=FALSE}
mod2 <- brm(
  Low ~ LWT + Smoker, data = dat, 
  family = bernoulli(),
  prior = c(
    set_prior("normal(0, 0.4)", "b", coef = "LWT"),
    set_prior("normal(0, 2)", "b", coef = "SmokerTRUE")
  ))
```


```{r, echo=FALSE, eval=FALSE}
saveRDS(mod2, "data/mod2.rds")
```

```{r, echo=FALSE}
mod2 <- readRDS("data/mod2.rds")
```

---

# (e) Compare to other more or less complicated models

```{r, eval=TRUE, dev='svg', fig.height=4}
plot(mod2)
```


---

# (e) Leave-one-out Cross-validation

```{r}
loo1 <- loo(mod, model_names = "LWT")
loo2 <- loo(mod2, model_names = "LWT+Smoker")
loo_compare(loo1, loo2)
```


---

# Examine the model outputs

```{r}
conditional_effects(mod)
```

---

# Examine the model outputs

```{r}
conditional_effects(
  mod2, effects = "LWT",
  conditions = data.frame(Smoker = c(FALSE, TRUE),
                          cond__ = c("Non-smoker", "Smoker")))
```

---

# Posterior intervals

```{r}
mcmc_plot(mod2)
```

---

# Joint Posterior Density

```{r, dev='svg', fig.height=4, fig.width=6}
mcmc_plot(mod2, variable = c("b_LWT", "b_SmokerTRUE"), 
          type = "scatter")
```

---

# Joint Posterior Density

```{r, dev='svg', fig.height=4, fig.width=6}
mcmc_plot(mod2, type = "pairs")
```

---

# Joint Posterior Density

What's the probability the both $\beta_{LWT} > 0$ and $\beta_{Smoker} < 0$, i.e., that smoking is protective and the probability of low birthweight increases with mother's weight?

```{r}
df <- as_draws_df(mod2)
head(df)
```


---

# Joint Posterior Density

Let's do this carefully. First select columns.

```{r}
df %>% 
  as_tibble() %>% 
 select(b_LWT, b_SmokerTRUE)
```


---

# Joint Posterior Density

Label the ones that meet our criteria.

```{r}
df %>% 
  as_tibble() %>% 
  select(b_LWT, b_SmokerTRUE) %>% 
  mutate(meets_criteria = b_LWT > 0 & b_SmokerTRUE < 0)
```


---

# Joint Posterior Density

Find the proportion of samples from posterior distribution that meets the criteria.

```{r}
df %>% 
  as_tibble() %>% 
  select(b_LWT, b_SmokerTRUE) %>% 
  mutate(meets_criteria = b_LWT > 0 & b_SmokerTRUE < 0) %>% 
  summarise(proportion = mean(meets_criteria))
```

---

# Joint Posterior Density

What's the probability that the odds ratio associated with a 1 kg difference in mother's weight is between 0.99 and 1.01?

```{r}
df %>% 
  as_tibble() %>% 
  select(b_LWT) %>% 
  mutate(
    odds_ratio = exp(b_LWT),
    meets_criteria = between(odds_ratio, .99, 1.01))
```


---

# Joint Posterior Density

Find proportion that meets criteria

```{r}
df %>% 
  as_tibble() %>% 
  select(b_LWT) %>% 
  mutate(
    odds_ratio = exp(b_LWT),
    meets_criteria = between(odds_ratio, .99, 1.01)) %>% 
  summarise(proportion = mean(meets_criteria))
```

This is the posterior probability that the odds ratio is between 0.99 and 1.01.


---

# Bayes Factors



---

# Bayes Factors

<center>
<img src="figures/edu_brain.jpg" height=400>
</center>

.footnote[Nyberg, Lars, Fredrik Magnussen, Anders Lundquist, William Baaré, David Bartrés-Faz, Lars Bertram, C. J. Boraxbekk, et al. “Educational Attainment Does Not Influence Brain Aging.” Proceedings of the National Academy of Sciences 118, no. 18 (May 4, 2021). https://doi.org/10.1073/pnas.2101644118.]

---

class: inverse, middle, center

# Was all of this worth the effort?

---

# Bayesian Models can Easily be Extended

Next slides give some examples.

---

# Covariate measurement error

Linear regression model. 

$$y = \beta_{0} + x \beta + \epsilon$$

$x$ is measured with error, so we don't see it. Instead we have $w$, defined by

$$w = x + \delta$$

This is a straightforward extension in the Bayesian setting.


---

# Missing Data

Treat missing data as parameters and sample them within the Metropolis-Hastings algorithm.


---

# Hierarchical Models

- Longitudinal designs with repeated measurements

- Other grouping structures (students nested in classes nested in schools)


---

# Philosophical Issues



---



---

class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](https://yihui.org/knitr/), and [R Markdown](https://rmarkdown.rstudio.com).
