<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>EPINOR Annual Meeting 2021</title>
    <meta charset="utf-8" />
    <meta name="author" content="Øystein Sørensen" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# EPINOR Annual Meeting 2021
## Part III - Bayesian analysis with R
### Øystein Sørensen
### University of Oslo
### 2021/11/02

---








# Outline for part III

- An overview of tools for Bayesian analysis in R

- The 'brms' package: probably the best place to start

- 15 minute break

- Hands-on exercises

---

# Fitting Bayesian models

.pull-left[
- Computation for Bayesian models requires more expertise than fitting frequentist models. 

- Series of papers by Greenland written 10-15 years ago contain lots of tricks for approximately fitting Bayesian models using familiar software.

- In contrast, fairly complex frequentist models can be estimated with a single line of code or the press of a button, e.g., mixed models, Cox regression.

]
.pull-right[

&lt;img src="figures/greenland1.jpg" width=400&gt;

&lt;img src="figures/greenland2.jpg" width=400&gt;

&lt;img src="figures/greenland3.jpg" width=400&gt;
]

---

# Software for fitting Bayesian models

- Several attempts at making this process easier.

--

- [BUGS - Bayesian Inference Using Gibbs Sampling](https://www.mrc-bsu.cam.ac.uk/software/bugs/)
  
  - Early software, which you may encounter in older textbooks. Can be used, but no longer actively developed.
  
- [JAGS - Just Another Gibbs Sampler](https://mcmc-jags.sourceforge.io/)

  - Further development of BUGS, but no new releases last four years.
  
- [Stan](https://mc-stan.org/)

  - State-of-the-art tool for Bayesian computation, developed mainly at Columbia University.
  
  - Very active community.


---

# Fitting Bayesian models

- Stan is a programming language of its own, definitely increasing the barrier for easy learning.

- Can use R packages that yield familiar syntax, translating R code into Stan code:

  - [rethinking](https://github.com/rmcelreath/rethinking)
  
  - [rstanarm](https://mc-stan.org/rstanarm/)
  
  - [brms](https://paul-buerkner.github.io/brms/)
  
- [You can also use Stan from Stata!](https://github.com/stan-dev/statastan)


---

# Stan?

.pull-left[
[Stanisław Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam)

&gt; a Polish scientist in the fields of mathematics and nuclear physics. He participated in the Manhattan Project, originated the Teller–Ulam design of thermonuclear weapons, discovered the concept of the cellular automaton, invented the Monte Carlo method of computation, and suggested nuclear pulse propulsion
]
.pull-right[
&lt;img src="figures/Stanislaw_Ulam.jpg" height=400&gt;
]


---

# brms


```r
library(brms)
```

&lt;center&gt;
&lt;img src="figures/brms_paper.jpg" width=400&gt;
&lt;/center&gt;

.footnote[
Bürkner, Paul-Christian. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80, no. 1 (August 29, 2017): 1–28. https://doi.org/10.18637/jss.v080.i01.
Bürkner, Paul-Christian. “Advanced Bayesian Multilevel Modeling with the R Package Brms.” The R Journal 10, no. 1 (2018): 395–411. https://doi.org/10.32614/RJ-2018-017.
]

---

# Fitting a logistic regression model with brms


```r
dat &lt;- readRDS("data/bwt.rds")
head(dat)
```

```
## # A tibble: 6 × 5
##     Low   Age   LWT Smoker Hypertension
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;  &lt;lgl&gt;       
## 1     0    19  82.6 FALSE  FALSE       
## 2     0    33  70.3 FALSE  FALSE       
## 3     0    20  47.6 TRUE   FALSE       
## 4     0    21  49.0 TRUE   FALSE       
## 5     0    18  48.5 TRUE   FALSE       
## 6     0    21  56.2 FALSE  FALSE
```


---

# Fitting a logistic regression model with brms

```r
mod &lt;- brm(Low ~ LWT, data = dat, family = bernoulli())
```
--
```r
## Compiling Stan program...
## Start sampling
## 
## SAMPLING FOR MODEL 'd3c8392ac3fcb0c96e96bdaad5582435' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## ...
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.058381 seconds (Warm-up)
## Chain 4:                0.054892 seconds (Sampling)
## Chain 4:                0.113273 seconds (Total)
## Chain 4: 

```


---

# Fitting a logistic regression model with brms



```r
mod &lt;- brm(Low ~ LWT, data = dat, family = bernoulli())
```

is very similar to


```r
mod_glm &lt;- glm(Low ~ LWT, data = dat, family = binomial())
```

- `family = binomial()` would work with brms too, but is not the standard syntax.

---

# Model output







```r
summary(mod)
```

```
##  Family: bernoulli 
##   Links: mu = logit 
## Formula: Low ~ LWT 
##    Data: dat (Number of observations: 189) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.05      0.81    -0.53     2.62 1.00     3397     2454
## LWT          -0.03      0.01    -0.06    -0.00 1.00     2916     2432
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

class: inverse, middle, center

# Let's try to use the Bayesian workflow

---

# (a) Exploratory data analysis

- Important, but not particularly related to brms. 

- But if you want to get good at data visualization in R, there are lots of good and free resources at https://ggplot2.tidyverse.org/

- You'll also get some hands-on experience in the exercises.

---

# (b) Prior distribution

In the previous lecture we decided on using a normal prior for `\(\beta_{LWT}\)` with mean 0 and standard deviation 0.4.

We start by getting the default prior:


```r
prior &lt;- get_prior(Low ~ LWT, family = bernoulli(), 
                   data = dat)
print(prior, show_df = FALSE)
```

```
## b ~ (flat)
## b_LWT ~ (flat)
## Intercept ~ student_t(3, 0, 2.5)
```

- "b" denotes all population-level parameters
- "b_LWT" denotes the particular LWT parameter
- "Intercept" is intercept...

---

# (b) Prior distribution

Update to get what we want:


```r
prior$prior[2] &lt;- "normal(0, 0.4)"
```

--

Check that prior is updated:


```r
print(prior, show_df = FALSE)
```

```
## b ~ (flat)
## b_LWT ~ normal(0, 0.4)
## Intercept ~ student_t(3, 0, 2.5)
```

---

# (c) Check that the computations work correctly

Then we have to fit the model first, with our preferred prior:


```r
mod &lt;- brm(Low ~ LWT, data = dat, family = bernoulli(),
           prior = prior)
```






--

```
## Compiling Stan program...
## recompiling to avoid crashing R session
## Start sampling
## 
## SAMPLING FOR MODEL 'f53afefa5fd92239adec95c91ede857a' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.032954 seconds (Warm-up)
## Chain 1:                0.025207 seconds (Sampling)
## Chain 1:                0.058161 seconds (Total)
## Chain 1: 
## 
```


---

# (c) Check that computations work correctly


```r
plot(mod)
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-16-1.svg" style="display: block; margin: auto;" /&gt;


Most of the plotting functions in brms use the BayesPlot package, which has a [homepage with many nice examples of what can be done](https://mc-stan.org/bayesplot/).

---

# (c) Check that computations work correctly


```r
plot(mod, combo = c("parcoord", "rank_overlay"))
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-17-1.svg" style="display: block; margin: auto;" /&gt;


---

# (c) Check that computations work correctly


```r
summary(mod)
```

```
##  Family: bernoulli 
##   Links: mu = logit 
## Formula: Low ~ LWT 
##    Data: dat (Number of observations: 189) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.07      0.78    -0.42     2.63 1.00     3764     2987
## LWT          -0.03      0.01    -0.06    -0.01 1.00     3362     2936
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```


---

# (d) Posterior predictive checks


```r
pp_check(mod)
```

```
## Using 10 posterior draws for ppc type 'dens_overlay' by default.
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-19-1.svg" style="display: block; margin: auto;" /&gt;

A very fancy way of checking that the proportion of children with low birthweights is replicated by the model predictions.

---

# (d) Posterior predictive checks


```r
pp_check(mod, ndraws = 100)
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-20-1.svg" style="display: block; margin: auto;" /&gt;

---

# (d) Error histograms


```r
pp_check(mod, type = "error_hist", ndraws = 12, binwidth = 1)
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-21-1.svg" style="display: block; margin: auto;" /&gt;


---

# (d) Error histograms


.pull-left[

- Low birthweight is coded as 1 and normal as 0.

- The table shows what `\(y - y_{\text{rep}}\)` means:

| `\(y - y_{\text{rep}}\)` | Predicted low bw `\(y_{\text{rep}}=1\)` | Predicted normal bw `\(y_{\text{rep}}=0\)` |
| ---- | ---- | ---- |
| Actual low bw `\(y=1\)` | 0 | 1 | 
| Actual normal bw `\(y=0\)` | -1 | 0 | 

]
.pull-right[

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-22-1.svg" style="display: block; margin: auto;" /&gt;

]


---

# (d) Scatter plot

True value on y-axis, average prediction on x-axis.


```r
pp_check(mod, type = "scatter_avg")
```

```
## Using all posterior draws for ppc type 'scatter_avg' by default.
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-23-1.svg" style="display: block; margin: auto;" /&gt;

---

# (d) Bar plot


```r
pp_check(mod, ndraws = 100, type = "bars")
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-24-1.svg" style="display: block; margin: auto;" /&gt;

True data in light blue, predictions from repeated sampling from the model overlayed.

---

# (e) Compare to other more or less complicated models

Extend the model by adding smoking status


```r
mod2 &lt;- brm(Low ~ LWT + Smoker, data = dat, 
            family = bernoulli())
```


---

# (e) Compare to other more or less complicated models

We need priors!


```r
mod2 &lt;- brm(
  Low ~ LWT + Smoker, data = dat, 
  family = bernoulli(),
  prior = c(
    set_prior(prior = "normal(0, 0.4)", class = "b", 
              coef = "LWT"),
    set_prior(prior = "normal(0, 2)", class = "b", 
              coef = "SmokerTRUE")
  ))
```

- Let's stick to this prior for smoking now. You will look further at it in the exercises.





---

# (e) Model with smoking


```r
plot(mod2)
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-29-1.svg" style="display: block; margin: auto;" /&gt;


---

# (e) Compare by leave-one-out cross-validation


```r
loo1 &lt;- loo(mod, model_names = "LWT")

loo2 &lt;- loo(mod2, model_names = "LWT+Smoker")

loo_compare(loo1, loo2)
```

```
##            elpd_diff se_diff
## LWT+Smoker  0.0       0.0   
## LWT        -1.1       2.1
```

- Model without smoking yields worse fit, but not "significantly". 

  - How can we tell?


--

- We keep both, and analyse their estimates.

---

# Examine the model outputs


```r
conditional_effects(mod)
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-31-1.svg" style="display: block; margin: auto;" /&gt;

---

# Examine the model outputs


```r
conditional_effects(
  mod2, effects = "LWT",
  conditions = data.frame(Smoker = c(FALSE, TRUE),
                          cond__ = c("Non-smoker", "Smoker")))
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-32-1.svg" style="display: block; margin: auto;" /&gt;

---

# Posterior intervals


```r
mcmc_plot(mod2)
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-33-1.svg" style="display: block; margin: auto;" /&gt;

---

# Joint posterior density


```r
mcmc_plot(mod2, variable = c("b_LWT", "b_SmokerTRUE"), 
          type = "scatter")
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-34-1.svg" style="display: block; margin: auto;" /&gt;

---

# Joint posterior density


```r
mcmc_plot(mod2, type = "pairs")
```

&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-35-1.svg" style="display: block; margin: auto;" /&gt;

---

# Joint posterior density

What's the probability the both `\(\beta_{LWT} &gt; 0\)` and `\(\beta_{Smoker} &lt; 0\)`, i.e., that smoking is protective and the probability of low birthweight increases with mother's weight?


```r
df &lt;- as_draws_df(mod2)
head(df)
```

```
## # A draws_df: 6 iterations, 1 chains, and 4 variables
##   b_Intercept   b_LWT b_SmokerTRUE lp__
## 1       -0.87 -0.0010         0.68 -119
## 2        1.65 -0.0512         1.19 -118
## 3        1.20 -0.0400         0.43 -117
## 4        1.08 -0.0440         0.67 -119
## 5        1.36 -0.0482         0.75 -118
## 6       -0.20 -0.0094         0.68 -119
## # ... hidden reserved variables {'.chain', '.iteration', '.draw'}
```


---

# Joint posterior density

Let's do this carefully. First select columns.


```r
df %&gt;% 
  as_tibble() %&gt;% 
  select(b_LWT, b_SmokerTRUE)
```

```
## # A tibble: 4,000 × 2
##       b_LWT b_SmokerTRUE
##       &lt;dbl&gt;        &lt;dbl&gt;
##  1 -0.00101       0.680 
##  2 -0.0512        1.19  
##  3 -0.0400        0.434 
##  4 -0.0440        0.673 
##  5 -0.0482        0.747 
##  6 -0.00941       0.678 
##  7 -0.0225        0.776 
##  8 -0.0291        0.795 
##  9 -0.0261        0.320 
## 10 -0.0480       -0.0360
## # … with 3,990 more rows
```


---

# Joint posterior density

Label the ones that meet our criteria.


```r
df %&gt;% 
  as_tibble() %&gt;% 
  select(b_LWT, b_SmokerTRUE) %&gt;% 
  mutate(meets_criteria = b_LWT &gt; 0 &amp; b_SmokerTRUE &lt; 0)
```

```
## # A tibble: 4,000 × 3
##       b_LWT b_SmokerTRUE meets_criteria
##       &lt;dbl&gt;        &lt;dbl&gt; &lt;lgl&gt;         
##  1 -0.00101       0.680  FALSE         
##  2 -0.0512        1.19   FALSE         
##  3 -0.0400        0.434  FALSE         
##  4 -0.0440        0.673  FALSE         
##  5 -0.0482        0.747  FALSE         
##  6 -0.00941       0.678  FALSE         
##  7 -0.0225        0.776  FALSE         
##  8 -0.0291        0.795  FALSE         
##  9 -0.0261        0.320  FALSE         
## 10 -0.0480       -0.0360 FALSE         
## # … with 3,990 more rows
```


---

# Joint posterior density

Find the proportion of samples from posterior distribution that meets the criteria.


```r
df %&gt;% 
  as_tibble() %&gt;% 
  select(b_LWT, b_SmokerTRUE) %&gt;% 
  mutate(meets_criteria = b_LWT &gt; 0 &amp; b_SmokerTRUE &lt; 0) %&gt;% 
  summarise(proportion = mean(meets_criteria))
```

```
## # A tibble: 1 × 1
##   proportion
##        &lt;dbl&gt;
## 1    0.00025
```

---

# Joint posterior density

What's the probability that the odds ratio associated with a 1 kg difference in mother's weight is between 0.99 and 1.01?


```r
df %&gt;% 
  as_tibble() %&gt;% 
  select(b_LWT) %&gt;% 
  mutate(
    odds_ratio = exp(b_LWT),
    meets_criteria = between(odds_ratio, .99, 1.01))
```

```
## # A tibble: 4,000 × 3
##       b_LWT odds_ratio meets_criteria
##       &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;         
##  1 -0.00101      0.999 TRUE          
##  2 -0.0512       0.950 FALSE         
##  3 -0.0400       0.961 FALSE         
##  4 -0.0440       0.957 FALSE         
##  5 -0.0482       0.953 FALSE         
##  6 -0.00941      0.991 TRUE          
##  7 -0.0225       0.978 FALSE         
##  8 -0.0291       0.971 FALSE         
##  9 -0.0261       0.974 FALSE         
## 10 -0.0480       0.953 FALSE         
## # … with 3,990 more rows
```


---

# Joint posterior density

Find proportion that meets criteria


```r
df %&gt;% 
  as_tibble() %&gt;% 
  select(b_LWT) %&gt;% 
  mutate(
    odds_ratio = exp(b_LWT),
    meets_criteria = between(odds_ratio, .99, 1.01)
    ) %&gt;% 
  summarise(proportion = mean(meets_criteria))
```

```
## # A tibble: 1 × 1
##   proportion
##        &lt;dbl&gt;
## 1     0.0532
```

This is the posterior probability that the odds ratio is between 0.99 and 1.01.


---

# Bayes factors

- A Bayesian alternative to hypothesis testing: compare the posterior probabilities of two models.


```r
bayes_factor(mod, mod2)
```

- But Bayes factors work both ways.

  - High posterior probability of `mod` means effect of `Smoker` is close to zero.
  
  - High posterior probability of `mod2` means effect of `Smoker` is not zero.
  
- Practical use of Bayes factors often criticized by statistics. In particular, the answers may be very sensitive to the choice of priors.

---

# Bayes factors - proving the null

&lt;center&gt;
&lt;img src="figures/nyberg_edu.jpg" width=500&gt;
&lt;img src="figures/edu_brain.jpg" height=300&gt;
&lt;/center&gt;

.footnote[Nyberg, Lars, Fredrik Magnussen, Anders Lundquist, William Baaré, David Bartrés-Faz, Lars Bertram, C. J. Boraxbekk, et al. “Educational Attainment Does Not Influence Brain Aging.” Proceedings of the National Academy of Sciences 118, no. 18 (May 4, 2021). https://doi.org/10.1073/pnas.2101644118.]

---

# Bayes factors - proving the null

From the Nyberg et al. paper:

&gt; The informative prior assigned a very large prior probability that the effect of interest was close to zero (SD = 0.5 mm3/y). The obtained BF01 = 1.29 implies that the posterior probability is even more concentrated around zero than the informative prior, thus favoring the null hypothesis.


Table for recommended interpretation following Jeffreys (1961) [can be found on Wikipedia](https://en.wikipedia.org/wiki/Bayes_factor).


.footnote[Jeffreys, Harold (1998) [1961]. The Theory of Probability (3rd ed.). Oxford, England. p. 432. ISBN 9780191589676.]

---


class: inverse, middle, center

# Was all of this worth the effort?

---

# Was all of this worth the effort?

Maybe not in this particular case, but Bayesian models can easily be extended.


---

# Data integration

Assume we got other birthweight dataset, from Norway, South Africa, Chile, Japan. Assuming that the effects are the same in all of these populations seems a bit strong.

We could "easily" extend the logistic regression to a hierarchical model:

- `\(\beta_{LWT}\)`, average effect of birthweight "in the world".

- `\(\beta_{a,LWT}\)`, effect of birthweight in country `\(a\)`. We would add an additional layer of hyperpriors to reflect that each subpopulation deviates from the whole world.

Something like this would get us started:


```r
mod &lt;- brm(Low ~ LWT + Smoker + (LWT + Smoker | Country),
           data = dat, family = bernoulli())
```



---

# Covariate measurement error

.pull-left[
Regression model. 

`$$y = \beta_{0} + x \beta + \epsilon$$`

`\(x\)` is measured with error, so we don't see it. Instead we have `\(w\)`, defined by

`$$w = x + \delta$$`

(Measured = True + Noise)

Using measurements `\(w\)` lead to bias in `\(\beta\)`. Quite straightforward to take into account in Bayesian models.

- Treat true value `\(x\)` as an unknown parameter, just like `\(\beta\)`. Sample it from the posterior.
]

.pull-right[

&lt;img src="figures/bp_measure.webp"&gt;

]



---

# Missing data

What if data are missing?

Treat the missing data as parameters. Sample them from posterior together with all other parameters.

See, e.g., [the mice package](https://CRAN.R-project.org/package=mice).


---

# Longitudinal data

- Longitudinal designs with repeated measurements



.pull-left[
Example in brms. Linear mixed model with random slopes.


```r
mod &lt;- brm(
  y ~ age + (age | id), 
  data = dat)
```


- Can study level-slope correlation: Do people who start out high grow faster or slower?



]

.pull-right[


&lt;img src="part-iii-brms-and-stan_files/figure-html/unnamed-chunk-45-1.png" style="display: block; margin: auto;" /&gt;


]




---

# Philosophical issues

- Statistical methods and analysis workflows relate the philosophy of science.

- Frequentist methods associated with *deductive inference*, and if practiced correctly, ensure long-term error control at the `\(\alpha\)` level.

- Bayesian methods have been identified as *inductive inference*. However, Gelman and Shalizi (2013) argue that Bayesian inference as it is used by practitioners, is *deductive inference*.

- If interested in these issues, see Mayo (2018) for an impressive book.

.footnote[
Gelman, Andrew, and Cosma Rohilla Shalizi. “Philosophy and the Practice of Bayesian Statistics.” British Journal of Mathematical and Statistical Psychology 66, no. 1 (2013): 8–38. https://doi.org/10.1111/j.2044-8317.2011.02037.x.

Mayo, Deborah G. Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars. 1st ed. Cambridge, UK: Cambridge University Press, 2018.
]

---

class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](https://yihui.org/knitr/), and [R Markdown](https://rmarkdown.rstudio.com).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
